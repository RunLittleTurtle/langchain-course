{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AsMFKCALr3y"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/langchain-course/blob/main/chapters/07-lcel.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PQji83qLpVC"
      },
      "source": [
        "#### LangChain Essentials Course"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22dLBWVZLpVD"
      },
      "source": [
        "# LangChains Expression Language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiRUqmfBLpVE"
      },
      "source": [
        "LangChain is one of the most popular open source libraries for AI Engineers. It's goal is to abstract away the complexity in building AI software, provide easy-to-use building blocks, and make it easier when switching between AI service providers.\n",
        "\n",
        "In this example, we will introduce LangChain's Expression Langauge (LCEL), abstracting a full chain and understanding how it will work. We'll provide examples for both OpenAI's `gpt-4o-mini` *and* Meta's `llama3.2` via Ollama!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i690tIabLwb_",
        "outputId": "6fa3f33f-5738-4973-8dde-8a8f09691805",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/437.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m430.1/437.6 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.6/437.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.3/360.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "  langchain-core \\\n",
        "  langchain-openai\\\n",
        "  langchain-community \\\n",
        "  langsmith \\\n",
        "  docarray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOR87lEpLpVE"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ We will be using OpenAI for this example allowing us to run everything via API. If you would like to use Ollama instead, check out the [Ollama LangChain Course](https://github.com/aurelio-labs/langchain-course/tree/main/notebooks/ollama).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfpD-R6sLpVE"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ If using LangSmith, add your API key below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "exBqQHgqLpVE",
        "outputId": "396b68a6-a1cb-4757-c891-b8f0cb089e9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter LangSmith API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") or \\\n",
        "    getpass(\"Enter LangSmith API Key: \")\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"aurelioai-langchain-course-lcel-openai\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYGeByKjLpVF"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQbgErgdLpVF"
      },
      "source": [
        "## Traditional Chains vs LCEL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyluns_ELpVF"
      },
      "source": [
        "In this section we're going to dive into a basic example using the traditional method for building chains before jumping into LCEL. We will build a pipeline where the user must input a specific topic, and then the LLM will look and return a report on the specified topic. Generating a _research report_ for the user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTZTTiK7LpVF"
      },
      "source": [
        "### Traditional LLMChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDHfbKZ7LpVF"
      },
      "source": [
        "The `LLMChain` is the simplest chain originally introduced in LangChain. This chain takes a prompt, feeds it into an LLM, and _optionally_ adds an output parsing step before returning the result.\n",
        "\n",
        "Let's see how we construct this using the traditional method, for this we need:\n",
        "\n",
        "* `prompt` — a `PromptTemplate` that will be used to generate the prompt for the LLM.\n",
        "* `llm` — the LLM we will be using to generate the output.\n",
        "* `output_parser` — an optional output parser that will be used to parse the structured output of the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Jyz2vWLoLpVF"
      },
      "outputs": [],
      "source": [
        "from langchain import PromptTemplate\n",
        "\n",
        "prompt_template = \"Give me a small report on {topic}\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=prompt_template\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rykro39YLpVF"
      },
      "source": [
        "For the LLM, we'll start by initializing our connection to the OpenAI API. We do need an OpenAI API key, which you can get from the [OpenAI platform](https://platform.openai.com/api-keys).\n",
        "\n",
        "We will use the `gpt-4o-mini` model with a `temperature` of `0.0`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Uvs7hILRLpVF",
        "outputId": "aa473ebb-6ec9-4e31-ea54-706698bfa93e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OpenAI API key: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\") \\\n",
        "    or getpass(\"Enter your OpenAI API key: \")\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model_name=\"gpt-4o-mini\",\n",
        "    temperature=0.0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhs6_3c4LpVF",
        "outputId": "7389cb2e-7170-4a0b-d5ea-81d7f789f3ba"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 9, 'total_tokens': 19, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0392822090', 'id': 'chatcmpl-BUIvSlHFoiyUpdYpvnVlNeISJsbD7', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--79b93df6-4173-472b-b36a-431ee93b5a03-0', usage_metadata={'input_tokens': 9, 'output_tokens': 10, 'total_tokens': 19, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "llm_out = llm.invoke(\"Hello there\")\n",
        "llm_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V39qjMpLLpVG"
      },
      "source": [
        "Then we define our output parser, this will be used to parse the output of the LLM. In this case, we will use the `StrOutputParser` which will parse the `AIMessage` output from our LLM into a single string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "46qTjklnLpVG"
      },
      "outputs": [],
      "source": [
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "z0uwxedBLpVG",
        "outputId": "9b46465d-0b2d-4029-d7f0-7d17a78ce0c9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello! How can I assist you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "out = output_parser.invoke(llm_out)\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mk4CUy52LpVG"
      },
      "source": [
        "Through the `LLMChain` class we can place each of our components into a linear `chain`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PrFk1J9dLpVG",
        "outputId": "1aae59db-8eae-4866-ec8d-4efe70fb51e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-32987ff2088c>:3: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjNsVMVpLpVG"
      },
      "source": [
        "Note that the `LLMChain` _was_ deprecated in LangChain `0.1.17`, the expected way of constructing these chains today is through LCEL, which we'll cover in a moment.\n",
        "\n",
        "We can `invoke` our `chain`, providing a `topic` that we'd like to be researched."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDDq8Cq7LpVG",
        "outputId": "919810ba-f92e-49f9-d9cc-e11e64de2f95"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'topic': 'retrieval augmented generation',\n",
              " 'text': \"### Report on Retrieval-Augmented Generation (RAG)\\n\\n#### Introduction\\nRetrieval-Augmented Generation (RAG) is an innovative approach that combines the strengths of information retrieval and natural language generation. This method enhances the capabilities of language models by allowing them to access external knowledge bases or documents during the generation process, thereby improving the accuracy and relevance of the generated content.\\n\\n#### Key Concepts\\n1. **Information Retrieval**: RAG utilizes a retrieval mechanism to fetch relevant documents or snippets from a large corpus of text. This step is crucial as it provides the model with up-to-date and contextually relevant information that may not be encoded in its training data.\\n\\n2. **Natural Language Generation**: After retrieving relevant information, the model generates responses or text based on both the retrieved content and the input query. This dual approach allows for more informed and contextually appropriate outputs.\\n\\n3. **Architecture**: RAG typically consists of two main components:\\n   - **Retriever**: This component identifies and retrieves relevant documents from a pre-defined knowledge base or dataset.\\n   - **Generator**: This component, often based on transformer architectures like BERT or GPT, generates coherent and contextually relevant text using the retrieved information.\\n\\n#### Advantages\\n- **Enhanced Accuracy**: By leveraging external knowledge, RAG can produce more accurate and factually correct responses, especially in domains where information is constantly evolving.\\n- **Contextual Relevance**: The integration of retrieval allows the model to maintain context and relevance, improving the quality of generated text.\\n- **Scalability**: RAG can be scaled to incorporate vast amounts of data, making it suitable for applications requiring extensive knowledge bases.\\n\\n#### Applications\\n- **Question Answering**: RAG is particularly effective in question-answering systems where users seek specific information that may not be present in the model's training data.\\n- **Chatbots and Virtual Assistants**: By providing real-time information, RAG enhances the conversational abilities of chatbots, making them more useful and informative.\\n- **Content Creation**: RAG can assist in generating articles, reports, or summaries by pulling in relevant data from various sources.\\n\\n#### Challenges\\n- **Complexity**: The integration of retrieval and generation components adds complexity to the system, requiring careful tuning and optimization.\\n- **Dependence on Quality of Data**: The effectiveness of RAG is heavily reliant on the quality and relevance of the retrieved documents. Poor retrieval can lead to inaccurate or irrelevant generation.\\n- **Latency**: The retrieval process can introduce latency, which may affect the responsiveness of applications, particularly in real-time scenarios.\\n\\n#### Conclusion\\nRetrieval-Augmented Generation represents a significant advancement in the field of natural language processing. By combining retrieval and generation, RAG enhances the ability of language models to produce accurate, relevant, and contextually rich text. As research and development in this area continue, RAG is poised to play a crucial role in various applications, from customer support to content generation, ultimately improving user experience and information accessibility.\"}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "result = chain.invoke(\"retrieval augmented generation\")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbHqc1qLLpVG"
      },
      "source": [
        "We can view a formatted version of this output using the `Markdown` display:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 997
        },
        "id": "hBxPHOZ-LpVG",
        "outputId": "069aa25f-a3ac-432c-cf08-58bbc2bf91d5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Report on Retrieval-Augmented Generation (RAG)\n\n#### Introduction\nRetrieval-Augmented Generation (RAG) is an innovative approach that combines the strengths of information retrieval and natural language generation. This method enhances the capabilities of language models by allowing them to access external knowledge bases or documents during the generation process, thereby improving the accuracy and relevance of the generated content.\n\n#### Key Concepts\n1. **Information Retrieval**: RAG utilizes a retrieval mechanism to fetch relevant documents or snippets from a large corpus of text. This step is crucial as it provides the model with up-to-date and contextually relevant information that may not be encoded in its training data.\n\n2. **Natural Language Generation**: After retrieving relevant information, the model generates responses or text based on both the retrieved content and the input query. This dual approach allows for more informed and contextually appropriate outputs.\n\n3. **Architecture**: RAG typically consists of two main components:\n   - **Retriever**: This component identifies and retrieves relevant documents from a pre-defined knowledge base or dataset.\n   - **Generator**: This component, often based on transformer architectures like BERT or GPT, generates coherent and contextually relevant text using the retrieved information.\n\n#### Advantages\n- **Enhanced Accuracy**: By leveraging external knowledge, RAG can produce more accurate and factually correct responses, especially in domains where information is constantly evolving.\n- **Contextual Relevance**: The integration of retrieval allows the model to maintain context and relevance, improving the quality of generated text.\n- **Scalability**: RAG can be scaled to incorporate vast amounts of data, making it suitable for applications requiring extensive knowledge bases.\n\n#### Applications\n- **Question Answering**: RAG is particularly effective in question-answering systems where users seek specific information that may not be present in the model's training data.\n- **Chatbots and Virtual Assistants**: By providing real-time information, RAG enhances the conversational abilities of chatbots, making them more useful and informative.\n- **Content Creation**: RAG can assist in generating articles, reports, or summaries by pulling in relevant data from various sources.\n\n#### Challenges\n- **Complexity**: The integration of retrieval and generation components adds complexity to the system, requiring careful tuning and optimization.\n- **Dependence on Quality of Data**: The effectiveness of RAG is heavily reliant on the quality and relevance of the retrieved documents. Poor retrieval can lead to inaccurate or irrelevant generation.\n- **Latency**: The retrieval process can introduce latency, which may affect the responsiveness of applications, particularly in real-time scenarios.\n\n#### Conclusion\nRetrieval-Augmented Generation represents a significant advancement in the field of natural language processing. By combining retrieval and generation, RAG enhances the ability of language models to produce accurate, relevant, and contextually rich text. As research and development in this area continue, RAG is poised to play a crucial role in various applications, from customer support to content generation, ultimately improving user experience and information accessibility."
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "display(Markdown(result[\"text\"]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cbmq0DfeLpVG"
      },
      "source": [
        "That is a simple `LLMChain` using the traditional LangChain method. Now let's move onto LCEL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWets8OpLpVG"
      },
      "source": [
        "## LangChain Expression Language (LCEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Lr5NA0xLpVG"
      },
      "source": [
        "**L**ang**C**hain **E**xpression **L**anguage (LCEL) is the recommended approach to building chains in LangChain. Having superceeded the traditional methods with `LLMChain`, etc. LCEL gives us a more flexible system for building chains. The pipe operator `|` is used by LCEL to _chain_ together components. Let's see how we'd construct an `LLMChain` using LCEL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8R-9b7ulLpVH"
      },
      "outputs": [],
      "source": [
        "lcel_chain = prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdJvzetzLpVH"
      },
      "source": [
        "We can `invoke` this chain in the same way as we did before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "XKWzIwpCLpVH",
        "outputId": "3d137151-573a-4d7e-8def-f01a0c1fa338"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'### Report on Retrieval-Augmented Generation (RAG)\\n\\n#### Introduction\\nRetrieval-Augmented Generation (RAG) is an innovative approach that combines the strengths of information retrieval and natural language generation. This method enhances the capabilities of language models by allowing them to access external knowledge sources, thereby improving the accuracy and relevance of generated responses.\\n\\n#### Concept Overview\\nRAG operates on the principle of integrating a retrieval mechanism with a generative model. The process typically involves two main components:\\n\\n1. **Retrieval Component**: This part of the system retrieves relevant documents or pieces of information from a large corpus based on a given query. It often employs techniques such as dense vector representations and similarity search to identify the most pertinent data.\\n\\n2. **Generation Component**: After retrieving relevant information, the generative model (often based on architectures like Transformers) synthesizes a coherent and contextually appropriate response. This model uses the retrieved documents as additional context to enhance its output.\\n\\n#### Advantages\\n- **Enhanced Knowledge Access**: RAG allows models to leverage vast external knowledge bases, making them more informed and capable of providing accurate answers.\\n- **Improved Contextuality**: By incorporating real-time data retrieval, RAG can generate responses that are more relevant to current events or specific user queries.\\n- **Reduced Hallucination**: Traditional generative models sometimes produce inaccurate or fabricated information (a phenomenon known as \"hallucination\"). RAG mitigates this risk by grounding responses in retrieved documents.\\n\\n#### Applications\\nRAG has a wide range of applications, including:\\n- **Question Answering**: Providing precise answers to user queries by retrieving relevant information from databases or the internet.\\n- **Chatbots and Virtual Assistants**: Enhancing conversational agents with up-to-date knowledge and contextually relevant responses.\\n- **Content Creation**: Assisting in generating articles, reports, or summaries by pulling in relevant data from various sources.\\n\\n#### Challenges\\nDespite its advantages, RAG faces several challenges:\\n- **Retrieval Quality**: The effectiveness of the system heavily relies on the quality of the retrieval component. Poor retrieval can lead to irrelevant or misleading information being used in generation.\\n- **Computational Complexity**: The dual process of retrieval and generation can be resource-intensive, requiring significant computational power and optimization.\\n- **Integration of Diverse Sources**: Ensuring that the retrieved information is coherent and contextually integrated into the generated text can be complex.\\n\\n#### Conclusion\\nRetrieval-Augmented Generation represents a significant advancement in the field of natural language processing. By effectively combining retrieval and generation, RAG systems can produce more accurate, relevant, and context-aware outputs. As research and development in this area continue, RAG is poised to play a crucial role in the evolution of intelligent systems and applications across various domains.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "result = lcel_chain.invoke(\"retrieval augmented generation\")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7E8_msHLpVH"
      },
      "source": [
        "The output format is slightly different, but the underlying functionality and content being output is the same. As before, we can view a formatted version of this output using the `Markdown` display:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 951
        },
        "id": "42uPzB1JLpVH",
        "outputId": "749ec24e-bac7-497c-fdfa-33d091a9c945"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Report on Retrieval-Augmented Generation (RAG)\n\n#### Introduction\nRetrieval-Augmented Generation (RAG) is an innovative approach that combines the strengths of information retrieval and natural language generation. This method enhances the capabilities of language models by allowing them to access external knowledge sources, thereby improving the accuracy and relevance of generated responses.\n\n#### Concept Overview\nRAG operates on the principle of integrating a retrieval mechanism with a generative model. The process typically involves two main components:\n\n1. **Retrieval Component**: This part of the system retrieves relevant documents or pieces of information from a large corpus based on a given query. It often employs techniques such as dense vector representations and similarity search to identify the most pertinent data.\n\n2. **Generation Component**: After retrieving relevant information, the generative model (often based on architectures like Transformers) synthesizes a coherent and contextually appropriate response. This model uses the retrieved documents as additional context to enhance its output.\n\n#### Advantages\n- **Enhanced Knowledge Access**: RAG allows models to leverage vast external knowledge bases, making them more informed and capable of providing accurate answers.\n- **Improved Contextuality**: By incorporating real-time data retrieval, RAG can generate responses that are more relevant to current events or specific user queries.\n- **Reduced Hallucination**: Traditional generative models sometimes produce inaccurate or fabricated information (a phenomenon known as \"hallucination\"). RAG mitigates this risk by grounding responses in retrieved documents.\n\n#### Applications\nRAG has a wide range of applications, including:\n- **Question Answering**: Providing precise answers to user queries by retrieving relevant information from databases or the internet.\n- **Chatbots and Virtual Assistants**: Enhancing conversational agents with up-to-date knowledge and contextually relevant responses.\n- **Content Creation**: Assisting in generating articles, reports, or summaries by pulling in relevant data from various sources.\n\n#### Challenges\nDespite its advantages, RAG faces several challenges:\n- **Retrieval Quality**: The effectiveness of the system heavily relies on the quality of the retrieval component. Poor retrieval can lead to irrelevant or misleading information being used in generation.\n- **Computational Complexity**: The dual process of retrieval and generation can be resource-intensive, requiring significant computational power and optimization.\n- **Integration of Diverse Sources**: Ensuring that the retrieved information is coherent and contextually integrated into the generated text can be complex.\n\n#### Conclusion\nRetrieval-Augmented Generation represents a significant advancement in the field of natural language processing. By effectively combining retrieval and generation, RAG systems can produce more accurate, relevant, and context-aware outputs. As research and development in this area continue, RAG is poised to play a crucial role in the evolution of intelligent systems and applications across various domains."
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(Markdown(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbutFcpyLpVI"
      },
      "source": [
        "### How Does the Pipe Operator Work?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIjOY7VILpVI"
      },
      "source": [
        "Before moving onto other LCEL features, let's take a moment to understand what the pipe operator `|` is doing and _how_ it works.\n",
        "\n",
        "Functionality wise, the pipe tells you that whatever the _left_ side outputs will be fed as input into the _right_ side. In the example of `prompt | llm | output_parser`, we see that `prompt` feeds into `llm` feeds into `output_parser`.\n",
        "\n",
        "The pipe operator is a way of chaining together components, and is a way of saying that whatever the _left_ side outputs will be fed as input into the _right_ side.\n",
        "\n",
        "Let's make a basic class named `Runnable` that will transform our a provided function into a _runnable_ class that we will then use with the pipe `|` operator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9xTvwvF4LpVI"
      },
      "outputs": [],
      "source": [
        "class Runnable:\n",
        "    def __init__(self, func):\n",
        "        self.func = func\n",
        "    def __or__(self, other):\n",
        "        def chained_func(*args, **kwargs):\n",
        "            return other.invoke(self.func(*args, **kwargs))\n",
        "        return Runnable(chained_func)\n",
        "    def invoke(self, *args, **kwargs):\n",
        "        return self.func(*args, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2Vg_PhXLpVI"
      },
      "source": [
        "With the `Runnable` class, we will be able wrap a function into the class, allowing us to then chain together multiple of these _runnable_ functions using the `__or__` method.\n",
        "\n",
        "First, let's create a few functions that we'll chain together:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "G7HCw9-VLpVI"
      },
      "outputs": [],
      "source": [
        "def add_five(x):\n",
        "    return x+5\n",
        "\n",
        "def sub_five(x):\n",
        "    return x-5\n",
        "\n",
        "def mul_five(x):\n",
        "    return x*5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0fDvmtWLpVI"
      },
      "source": [
        "Now we wrap our functions with the `Runnable`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LfnBBcbxLpVI"
      },
      "outputs": [],
      "source": [
        "add_five_runnable = Runnable(add_five)\n",
        "sub_five_runnable = Runnable(sub_five)\n",
        "mul_five_runnable = Runnable(mul_five)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jHLbl_7LpVI"
      },
      "source": [
        "Finally, we can chain these together using the `__or__` method from the `Runnable` class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHFo61TwLpVI",
        "outputId": "1ef46821-e158-437e-b913-d26b36e6b7f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "chain = (add_five_runnable).__or__(sub_five_runnable).__or__(mul_five_runnable)\n",
        "\n",
        "chain.invoke(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oim1zWHLpVI"
      },
      "source": [
        "So we can see that we're able to chain together our functions using `__or__`. The pipe `|` operator is simply a shortcut for the `__or__` method, so we can create the exact same chain like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uh3-jF8pLpVJ",
        "outputId": "292ec5dc-1122-442b-90fe-fa6c941b5253"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "chain = add_five_runnable | sub_five_runnable | mul_five_runnable\n",
        "\n",
        "chain.invoke(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lnj00T8LpVJ"
      },
      "source": [
        "## LCEL `RunnableLambda`\n",
        "\n",
        "The `RunnableLambda` class is LangChain's built-in method for constructing a _runnable_ object from a function. That is, it does the same thing as the custom `Runnable` class we created earlier. Let's try it out with the same functions as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BmMH8GzVLpVJ"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "add_five_runnable = RunnableLambda(add_five)\n",
        "sub_five_runnable = RunnableLambda(sub_five)\n",
        "mul_five_runnable = RunnableLambda(mul_five)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkpwIKq6LpVJ"
      },
      "source": [
        "We chain these together again with the pipe `|` operator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9e58vaSELpVJ"
      },
      "outputs": [],
      "source": [
        "chain = add_five_runnable | sub_five_runnable | mul_five_runnable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FdqbdAyLpVJ"
      },
      "source": [
        "And call them using the `invoke` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rqHBMT8ULpVJ",
        "outputId": "76e53d69-00b5-4149-b4a2-6d0e833c0a91"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "chain.invoke(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47mu4xxqLpVJ"
      },
      "source": [
        "Now we want to try something a little more testing, so this time we will generate a report, and we will try and edit that report using this functionallity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xRJLzshqLpVJ"
      },
      "outputs": [],
      "source": [
        "prompt_str = \"give me a small report about {topic}\"\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=prompt_str\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "lfl_s4VALpVJ"
      },
      "outputs": [],
      "source": [
        "chain = prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 940
        },
        "id": "WSMlRM8wLpVJ",
        "outputId": "d3993d1a-393e-41e0-fe0e-8763035a9830"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "### Report on Artificial Intelligence (AI)\n\n#### Introduction\nArtificial Intelligence (AI) refers to the simulation of human intelligence in machines programmed to think and learn like humans. It encompasses a variety of technologies and methodologies, including machine learning, natural language processing, robotics, and computer vision. AI has rapidly evolved over the past few decades, becoming an integral part of various industries and daily life.\n\n#### Current State of AI\nAs of 2023, AI technologies are being utilized across multiple sectors, including healthcare, finance, transportation, and entertainment. Key advancements include:\n\n1. **Machine Learning (ML)**: Algorithms that enable computers to learn from and make predictions based on data. Deep learning, a subset of ML, has led to breakthroughs in image and speech recognition.\n\n2. **Natural Language Processing (NLP)**: Technologies that allow machines to understand and respond to human language. Applications include chatbots, virtual assistants, and language translation services.\n\n3. **Computer Vision**: The ability of machines to interpret and make decisions based on visual data. This technology is widely used in facial recognition, autonomous vehicles, and medical imaging.\n\n4. **Robotics**: AI-driven robots are increasingly used in manufacturing, logistics, and even healthcare, performing tasks ranging from assembly line work to surgery.\n\n#### Applications of AI\n- **Healthcare**: AI is used for diagnostics, personalized medicine, and predictive analytics, improving patient outcomes and operational efficiency.\n- **Finance**: AI algorithms analyze market trends, detect fraud, and automate trading, enhancing decision-making processes.\n- **Transportation**: Autonomous vehicles leverage AI for navigation and safety, while AI systems optimize traffic management and logistics.\n- **Entertainment**: Streaming services use AI to recommend content based on user preferences, while video games employ AI for more realistic and adaptive gameplay.\n\n#### Challenges and Ethical Considerations\nDespite its potential, AI poses several challenges:\n- **Bias and Fairness**: AI systems can perpetuate existing biases present in training data, leading to unfair outcomes.\n- **Privacy Concerns**: The use of AI in data collection raises significant privacy issues, particularly in surveillance and personal data usage.\n- **Job Displacement**: Automation driven by AI may lead to job losses in certain sectors, necessitating workforce retraining and adaptation.\n\n#### Future Outlook\nThe future of AI is promising, with ongoing research aimed at creating more generalizable and ethical AI systems. Innovations in explainable AI (XAI) seek to make AI decision-making processes more transparent. As AI continues to evolve, collaboration between technologists, policymakers, and ethicists will be crucial to harness its benefits while mitigating risks.\n\n#### Conclusion\nAI is transforming industries and society at large, offering significant opportunities for innovation and efficiency. However, it is essential to address the ethical and societal implications of AI to ensure its responsible development and deployment. As we move forward, a balanced approach will be key to maximizing the benefits of AI while minimizing its potential drawbacks."
          },
          "metadata": {}
        }
      ],
      "source": [
        "result = chain.invoke(\"AI\")\n",
        "display(Markdown(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE7HOxwaLpVJ"
      },
      "source": [
        "Here we are making two functions, `extract_fact` to pull out the main content of our text and `replace_word` that will replace AI with Skynet!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Wc7ws8FkLpVJ"
      },
      "outputs": [],
      "source": [
        "def extract_fact(x):\n",
        "    if \"\\n\\n\" in x:\n",
        "        return \"\\n\".join(x.split(\"\\n\\n\")[1:])\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "old_word = \"AI\"\n",
        "new_word = \"skynet\"\n",
        "\n",
        "def replace_word(x):\n",
        "    return x.replace(old_word, new_word)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktHrg5iSLpVK"
      },
      "source": [
        "Lets wrap these functions and see what the output is!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_vFeqMY-LpVK"
      },
      "outputs": [],
      "source": [
        "extract_fact_runnable = RunnableLambda(extract_fact)\n",
        "replace_word_runnable = RunnableLambda(replace_word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "TDWN-tQzLpVK"
      },
      "outputs": [],
      "source": [
        "chain = prompt | llm | output_parser | extract_fact_runnable | replace_word_runnable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 864
        },
        "id": "OCODP8vLLpVK",
        "outputId": "45abddc0-ccab-4dd6-a1a3-b8cb04ecaf58"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "#### Introduction\nRetrieval-Augmented Generation (RAG) is an innovative approach that combines the strengths of information retrieval and natural language generation. This method enhances the capabilities of language models by allowing them to access external knowledge sources, thereby improving the accuracy and relevance of generated responses.\n#### Concept Overview\nRAG operates on the principle of integrating a retrieval mechanism with a generative model. The process typically involves two main components:\n1. **Retrieval Component**: This part of the system retrieves relevant documents or pieces of information from a large corpus based on a given query. It utilizes techniques such as vector embeddings and similarity search to identify the most pertinent data.\n2. **Generation Component**: Once the relevant information is retrieved, the generative model (often based on architectures like Transformers) processes this data to produce coherent and contextually appropriate text. The model can leverage the retrieved information to enhance its responses, making them more informative and accurate.\n#### Advantages\n- **Enhanced Knowledge Access**: RAG allows models to tap into vast external knowledge bases, overcoming the limitations of fixed training data.\n- **Improved Accuracy**: By grounding responses in real-world data, RAG can provide more precise answers, especially for niche or specialized queries.\n- **Dynamic Learning**: The retrieval mechanism enables the model to stay updated with the latest information without requiring retraining.\n#### Applications\nRAG has a wide range of applications, including:\n- **Question Answering**: Providing accurate answers to user queries by retrieving relevant documents.\n- **Chatbots and Virtual Assistants**: Enhancing conversational agents with up-to-date information.\n- **Content Creation**: Assisting in generating articles, reports, or summaries based on retrieved data.\n#### Challenges\nDespite its advantages, RAG faces several challenges:\n- **Retrieval Quality**: The effectiveness of the system heavily relies on the quality of the retrieval component. Poor retrieval can lead to irrelevant or incorrect information being used in generation.\n- **Complexity**: Integrating retrieval and generation processes can complicate system design and increase computational requirements.\n- **Bias and Misinformation**: If the retrieval corpus contains biased or inaccurate information, it can adversely affect the generated output.\n#### Conclusion\nRetrieval-Augmented Generation represents a significant advancement in the field of natural language processing. By effectively combining retrieval and generation, RAG systems can produce more accurate, relevant, and contextually aware responses. As research and development in this area continue, RAG is poised to play a crucial role in the evolution of intelligent conversational agents and information systems. Future work will likely focus on improving retrieval mechanisms, addressing biases, and enhancing the overall efficiency of RAG systems."
          },
          "metadata": {}
        }
      ],
      "source": [
        "result = chain.invoke(\"retrieval augmented generation\")\n",
        "display(Markdown(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHSumR1tLpVK"
      },
      "source": [
        "Those are our `RunnableLambda` functions. It's worth noting that all inputs to these functions are expected to be a SINGLE arguments. If you have a function that accepts multiple arguments, you can input a dictionary with keys, then unpack them inside the function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIXKLyQKLpVK"
      },
      "source": [
        "## LCEL `RunnableParallel` and `RunnablePassthrough`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avFhxg_pLpVK"
      },
      "source": [
        "LCEL provides us with various `Runnable` classes that allow us to control the flow of data and execution order through our chains. Two of these are `RunnableParallel` and `RunnablePassthrough`.\n",
        "\n",
        "* `RunnableParallel` — allows us to run multiple `Runnable` instances in parallel. Acting almost as a Y-fork in the chain.\n",
        "\n",
        "* `RunnablePassthrough` — allows us to pass through a variable to the next `Runnable` without modification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTsqIAufLpVK"
      },
      "source": [
        "To see these runnables in action, we will create two data sources, each source provides specific information but to answer the question we will need both to fed to the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "zaD678FMLpVK",
        "outputId": "d0eb2792-0fae-4b98-de0f-bce4e1df42e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-7a4850ad4e22>:4: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
            "  embedding = OpenAIEmbeddings()\n",
            "/usr/local/lib/python3.11/dist-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
            "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
          ]
        }
      ],
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import DocArrayInMemorySearch\n",
        "\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "vecstore_a = DocArrayInMemorySearch.from_texts(\n",
        "    [\n",
        "        \"half the info is here\",\n",
        "        \"DeepSeek-V3 was released in December 2024\"\n",
        "    ],\n",
        "    embedding=embedding\n",
        ")\n",
        "vecstore_b = DocArrayInMemorySearch.from_texts(\n",
        "    [\n",
        "        \"the other half of the info is here\",\n",
        "        \"the DeepSeek-V3 LLM is a mixture of experts model with 671B parameters\"\n",
        "    ],\n",
        "    embedding=embedding\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ub4pjRrBLpVK"
      },
      "source": [
        "Here you can see the prompt does have three inputs, two for context and one for the question itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "pnxXrw-CLpVK"
      },
      "outputs": [],
      "source": [
        "prompt_str = \"\"\"Using the context provided, answer the user's question.\n",
        "Context:\n",
        "{context_a}\n",
        "{context_b}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "jL3YN1c1LpVK"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(prompt_str),\n",
        "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YchbLoKLpVK"
      },
      "source": [
        "Here we are wrapping our vector stores as retrievers so they can be fitted into one big retrieval variable to be used by the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "xkhFV8-SLpVL"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "\n",
        "retriever_a = vecstore_a.as_retriever()\n",
        "retriever_b = vecstore_b.as_retriever()\n",
        "\n",
        "retrieval = RunnableParallel(\n",
        "    {\n",
        "        \"context_a\": retriever_a, \"context_b\": retriever_b, \"question\": RunnablePassthrough()\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKtORU5xLpVL"
      },
      "source": [
        "The chain we'll be constructing will look something like this:\n",
        "\n",
        "![](https://github.com/aurelio-labs/langchain-course/blob/main/assets/lcel-flow.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "PNLpfS3_LpVL"
      },
      "outputs": [],
      "source": [
        "chain = retrieval | prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eo8BfprxLpVL"
      },
      "source": [
        "We `invoke` it as usual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "84gEXUG1LpVL",
        "outputId": "88142fdb-b4df-45ee-d2f2-a94c9b8b5f14"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The DeepSeek-V3 model, released in December 2024, is a mixture of experts model with 671 billion parameters.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "result = chain.invoke(\n",
        "    \"what architecture does the model DeepSeek released in december use?\"\n",
        ")\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcwr16jsLpVL"
      },
      "source": [
        "With that we've seen how we can use `RunnableParallel` and `RunnablePassthrough` to control the flow of data and execution order through our chains.\n",
        "\n",
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}